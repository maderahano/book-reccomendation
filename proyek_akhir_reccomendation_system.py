# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import precision_recall_fscore_support

"""# Data Understanding

## Data Loading
"""

books = pd.read_csv('Books.csv')
users = pd.read_csv('Users.csv')
ratings = pd.read_csv('Ratings.csv')

"""## Univariate Exploratory Data Analysis

### Book Variable
"""

# Melihat detail informasi dari dataset buku
books.info()

# Mengganti nama kolom agar lebih mudah pada saat pemanggilan data kolom
books.columns = ['ISBN', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher', 'Image_URL_S', 'Image_URL_M', 'Image_URL_L']
books.head()

"""Dapat dilihat dari informasi dataset buku diatas bahwa semua tipe data dari setiap kolom tersebut adalah object, tetapi khusus untuk kolom 'Year_Of_Publication' tersebut seharusnya isi nilai dari kolom terseubut berupa nilai yang tipe datanya integer saja, tetapi dari informasi dataset tersebut menunjukkan bahwa tipe data dari kolom Year_Of_Publication' adalah object, maka dari itu perlu ditelusuri lebih lanjut tentang data kolom ini yaitu dengan cara mengubah tipe data dari object menjadi int."""

books.Year_Of_Publication = books.Year_Of_Publication.astype('int')

"""Terdapat ValueError ketika mencoba mengubah tipe data dari object ke int. Pesan dari error tersebut menunjukkan bahwa terdapat beberapa data dari buku dataset yang memiliki value 'DK Publishing Inc' dari kolom 'Year_Of_Publication'. Maka dari itu, data tersebut harus dihapus dari dataset."""

# Melihat data buku dari kolom 'Year_Of_Publication' yang memiliki value 'DK Publishing Inc'
books[books.Year_Of_Publication == 'DK Publishing Inc']

# Menghapus data buku yang memiliki nilai 'DK Publishing Inc' dari kolom 'Year_Of_Publication'
books = books.drop(books[books.Year_Of_Publication == 'DK Publishing Inc'].index)
books[books.Year_Of_Publication == 'DK Publishing Inc']

# Lanjut melakukan casting
books.Year_Of_Publication = books.Year_Of_Publication.astype('int')

"""Ternyata setelah dilakuan casting lagi, masih ada value yang bukan mengandung tipe data int. Maka proses penghapus data dari langkah sebelumnya harus diulangi lagi"""

# Melihat data buku dari kolom 'Year_Of_Publication' yang memiliki value 'Gallimard'
books[books.Year_Of_Publication == 'Gallimard']

# Menghapus data buku yang memiliki nilai 'Gallimard' dari kolom 'Year_Of_Publication'
books = books.drop(books[books.Year_Of_Publication == 'Gallimard'].index)
books[books.Year_Of_Publication == 'Gallimard']

# Lanjut melakukan casting
books.Year_Of_Publication = books.Year_Of_Publication.astype('int')

# Melihat detail informasi dari dataset buku lagi
books.info()

# Menghitung 10 Penulis teratas dengan buku terbanyak
top_10_book_authors = books['Book_Author'].value_counts().head(10)
print(top_10_book_authors)

# Buat diagram batang
plt.figure(figsize=(10, 6))
top_10_book_authors.plot(kind='bar')
plt.title('10 Penulis Teratas dengan Buku Terbanyak')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45)
plt.show()

"""Selanjutnya dilakukan pendistribusian data untuk melihat 10 penulis teratas berdasarkan jumlah buku.

### User Variable
"""

# Melihat detail informasi dari dataset user
users.info()

# Mengganti nama kolom agar lebih mudah pada saat pemanggilan data kolom
users.columns = ['User_ID', 'Location', 'Age']
users.head()

"""Dilihat dari detail informasi dataset user tersebut bahwa semua tipe datasetnya normal walaupun tipe data dari age seharusnya int bukan float, tapi masalah tersebut dapat diabaikan. Selain itu kolom age memiliki nilai yang kosong dalam jumlah yang banyak, tetapi hal tersebut dapat diabaikan juga karena model yang saya pakai kedepannya bukan berdasarkan kondisi sosial dari setiap user tersebut.

### Rating Variable
"""

# Melihat detail informasi dari dataset rating
ratings.info()

# Mengganti nama kolom agar lebih mudah pada saat pemanggilan data kolom
ratings.columns = ['User_ID', 'ISBN', 'Book_Rating']
ratings.head()

"""Dilihat dari detail informasi dataset rating tersebut bahwa semua tipe datasetnya normal dan tidak ada missing value dari semua kolom yang ada pada dataset rating. Tetapi karena jumlah datanya sangat banyak yaitu 1.149.780, maka data tersebut harus dipangkas agar dapat menghemat waktu dalam melakukan training pada model Collaborative Filtering."""

collab_ratings = ratings[:25000]
collab_ratings

# Menggabungkan data books dan ratings berdasarkan ISBN
temp_books_ratings_df = pd.merge(books, ratings, on='ISBN')

# Menghitung jumlah rating yang didapatkan tiap buku oleh user
count_ratings_book = temp_books_ratings_df['ISBN'].value_counts().head(10)

# Mendapatkan 10 buku populer teratas berdasarkan jumlah peringkat
top_10_popular_books = books[books['ISBN'].isin(count_ratings_book.index)]
top_10_popular_books = top_10_popular_books.drop(labels=['Book_Author', 'Year_Of_Publication', 'Publisher', 'Image_URL_S', 'Image_URL_M', 'Image_URL_L'], axis=1).merge(count_ratings_book, on='ISBN').drop(labels=['ISBN'], axis=1).sort_values(by=['count'], ascending=False)
print(top_10_popular_books)

plt.figure(figsize=(10, 6))
plt.bar(top_10_popular_books['Book_Title'], top_10_popular_books['count'])
plt.title('10 Buku Populer Teratas')
plt.xlabel('Judul Buku')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Selanjutnya dilakukan pendistribusian data untuk melihat 10 buku paling populer dan 10 user paling berpengalaman."""

# Menggabungkan data books dan ratings berdasarkan User_ID
temp_users_ratings_df = pd.merge(users, ratings, on='User_ID')

# Menghitung jumlah rating yang diberikan oleh user ke buku
count_ratings_user = temp_users_ratings_df['User_ID'].value_counts().head(10)

# Mendapatkan 10 user yang berpengalaman berdasarkan jumlah peringkat
top_10_experienced_user = users[users['User_ID'].isin(count_ratings_user.index)]
top_10_experienced_user = top_10_experienced_user.drop(labels=['Location', 'Age'], axis=1).merge(count_ratings_user, on='User_ID').sort_values(by=['count'], ascending=False)
top_10_experienced_user.User_ID = top_10_experienced_user.User_ID.astype('str')
print(top_10_experienced_user)

plt.figure(figsize=(10, 6))
plt.bar(top_10_experienced_user['User_ID'], top_10_experienced_user['count'])
plt.title('10 User yang Paling Berpengalaman')
plt.xlabel('User ID')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Data Preparation

## Data Preprocessing

### Menggabungkan Data antara Book dengan Rating
"""

# Menggabungkan dataframe books dengan dataframe ratings berdasarkan ISBN
new_data = pd.merge(ratings, books, on='ISBN', how='left')

# Print dataframe new_data
new_data

"""## Data Preparation untuk Model Development Content-Based Filtering

### Mengatasi Missing Value
"""

# Mengecek missing value pada dataframe new_data
new_data.isnull().sum()

# Membersihkan missing value dengan fungsi dropna()
all_new_data_clean = new_data.dropna()
all_new_data_clean

# Mengecek missing value pada dataframe all_new_data_clean
all_new_data_clean.isnull().sum()

"""### Standarisasi Tipe Buku"""

# Mengurutkan data buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_data
fix_data = all_new_data_clean.sort_values('ISBN', ascending=True)
fix_data

# Mengecek berapa jumlah fix_data berdasarkan kolom 'ISBN'
len(fix_data.ISBN.unique())

# Membuat variabel preparation yang berisi dataframe fix_data kemudian mengurutkan berdasarkan ISBN
preparation = fix_data
preparation.sort_values('ISBN')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

# Mengonversi data series 'ISBN' menjadi dalam bentuk list
isbn = preparation.ISBN.tolist()

# Mengonversi data series 'Book_Title' menjadi dalam bentuk list
book_title = preparation.Book_Title.tolist()

# Mengonversi data series 'Book_Author' menjadi dalam bentuk list
book_author = preparation.Book_Author.tolist()

# Mengonversi data series 'Year_Of_Publication' menjadi dalam bentuk list
year_of_publication = preparation.Year_Of_Publication.tolist()

# Mengonversi data series 'Publisher' menjadi dalam bentuk list
publisher = preparation.Publisher.tolist()

# Mengonversi data series 'Image_URL_S' menjadi dalam bentuk list
image_url_s = preparation.Image_URL_S.tolist()

# Mengonversi data series 'Image_URL_M' menjadi dalam bentuk list
image_url_m = preparation.Image_URL_M.tolist()

# Mengonversi data series 'Image_URL_L' menjadi dalam bentuk list
image_url_l = preparation.Image_URL_L.tolist()

print(len(isbn))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))
print(len(image_url_s))
print(len(image_url_m))
print(len(image_url_l))

books_new = pd.DataFrame({
    'isbn': isbn,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher,
    'image_url_s': image_url_s,
    'image_url_m': image_url_m,
    'image_url_l': image_url_l

})

books_new

"""Karena jumlah dataset terlalu besar, dan alokasi memori yang digunakan akan sangat besar untuk memproses seluruh data dalam model development, untuk proyek ini, hanya menggunakan 30.000 data pertama yang akan digunakan."""

# Mengambil 25000 data pertama dari data books_new
books_new = books_new[:25000]

books_new

"""## Data Preparation untuk Model Development Collaborative Filtering dengan Popular Based

### Filter Data Buku Populer

Jumlah data buku yang populer dari dataset adalah sebanyak 50 buku
"""

# Kelompokan 'new_data' berdasarkan kolom 'Book_Title' dan jumlahkan total rating yang ada berdasarkan kolom 'Book_Rating' tiap judul buku tersebut
book_count_data = new_data.groupby('Book_Title').count()['Book_Rating'].reset_index()

# Ganti judul kolom 'Book_Rating' menjadi 'Count_Rating'
book_count_data.rename(columns={'Book_Rating':'Count_Rating'}, inplace=True)

# Print dataframe book_count_data
book_count_data

# Kelompokan 'new_data' berdasarkan kolom 'Book_Title' dan hitung rata-rata dari tiap judul buku tersebut berdasarkan rating yang ada dari kolom 'Book_Rating'
book_average_data = new_data.groupby('Book_Title').mean(numeric_only=True)['Book_Rating'].round(2).reset_index()

# Ganti judul kolom 'Book_Rating' menjadi 'Average_Rating'
book_average_data.rename(columns={'Book_Rating':'Average_Rating'}, inplace=True)

# Print dataframe average_rating
book_average_data

# Gabungkan data antara count_data dan average_data berdasarkan kolom 'Book_Title'
book_count_average_data = pd.merge(book_count_data, book_average_data, on='Book_Title', how='left')

# Print dataframe count_average_data
book_count_average_data

# Filter dan pilih 50 item yang paling popular berdasarkan rating rata-rata dengan minimal 250 rating dengan jumlah rating yang terbanyak
popular_book_data = book_count_average_data[book_count_average_data['Count_Rating'] >= 250].sort_values('Average_Rating', ascending=False).head(50)

# Print dataframe popular_data
popular_book_data.head(10)

"""### Filter Data User Berpengalaman

User yang berpengalaman ini dimaksudkan user yang telah memberi rating kepada buku sebanyak 200 buku
"""

# Kelompokan 'new_data' berdasarkan kolom 'User_ID' dan jumlahkan buku yang pernah diberi rating oleh User berdasarkan kolom 'Book_Rating'
# Hasilnya berupa tipe data Series dengan 'User_ID' sebagai index dan jumlah rating yang pernah diberi sebagai values
user_rating_count = new_data.groupby('User_ID').count()['Book_Rating']

# Print series user_count_data
user_rating_count

# Buat kondisi boolean dengan membandingkan setiap user yang telah memberi rating dengan 200, yang menghasilkan nilai True/False
# True berarti bahwa User telah memberi rating lebih dari 200 buku, sedangkan False menunjukkan bahwa User memberi rating kurang dari 200 buku
is_experienced_users = user_rating_count > 200

# Print series is_experienced_users
is_experienced_users

# Filter data 'user_rating_count' berdasarkan nilai True, sehingga hanya menghasilkan indeks 'User_ID' dari User yang telah memberi rating lebih dari 200 buku
experienced_users = user_rating_count[is_experienced_users].index

# Print user_count_data
experienced_users

# Filter data 'new_data' yang hanya menyimpan rating dari data 'experienced_users' berdasarkan kolom 'User_ID'
filtered_data = new_data[new_data['User_ID'].isin(experienced_users)]

# Print filtered_data
filtered_data

"""### Filter Data Buku Berdasarkan Dari Data User Berpengalaman dan Buku Populer"""

# Kelompokan 'filtered_data' berdasarkan kolom 'Book_Title' dan jumlahkan buku yang pernah diberi rating oleh User berdasarkan kolom 'Book_Rating'
# Hasilnya berupa tipe data Series dengan 'Book_Title' sebagai index dan jumlah rating yang pernah diberi sebagai values
book_rating_count = filtered_data.groupby('Book_Title').count()['Book_Rating']

# Print series book_rating_count
user_rating_count

# Buat kondisi boolean dengan membandingkan setiap buku yang telah diberi rating dengan 50, yang menghasilkan nilai True/False
# True berarti bahwa buku telah diberi rating lebih dari 50 rating, sedangkan False menunjukkan bahwa buku diberi rating kurang dari 50 rating
is_popular_book = book_rating_count >= 50

# Print series is_popular_book
is_popular_book

# Filter data 'book_rating_count' berdasarkan nilai True, sehingga hanya menghasilkan indeks 'Book_Titles' dari buku yang populer
popular_books = book_rating_count[is_popular_book].index

# Print popular_books
popular_books

# Filter data 'filtered_data' yang hanya menyimpan judul buku dari data 'popular_books' berdasarkan kolom 'User_ID'
final_data = filtered_data[filtered_data['Book_Title'].isin(popular_books)]

# Print final_data
final_data

"""### Pivot Dataset"""

# Buat tabel pivot untuk mengatur data rating buku
# Tabel pivot akan memiliki buku sebagai baris, user sebagai kolom, dan rating sebagai nilai sel

# Kelompokkan DataFrame 'final_data' berdasarkan 'Book_Title' dan 'User_ID'
# Hitung nilai rata-rata untuk setiap kombinasi buku dan user
# Tabel yang dihasilkan akan memiliki buku sebagai baris dan user sebagai kolom
pivot_data = final_data.pivot_table(index='Book_Title', columns='User_ID', values='Book_Rating')

# Print pivot_data
pivot_data

# Isi nilai yang hilang di tabel pivot_data dengan angka nol
pivot_data.fillna(0, inplace=True)

# Print pivot_data
pivot_data

"""## Data Preparation untuk Model Development Collaborative Filtering dengan User Based

### Encoding User ID
"""

# Mengubah User_ID menjadi list tanpa nilai yang sama
user_ids = collab_ratings.User_ID.unique().tolist()
print('List User_ID: ', user_ids)

# Melakukan encoding User_ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('Encoded User_ID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User_ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('Encoded Angka ke User_ID: ', user_encoded_to_user)

"""### Encoding ISBN"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_ids = collab_ratings.ISBN.unique().tolist()
print('List ISBN: ', isbn_ids)

# Melakukan proses encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}
print('Encoded ISBN : ', isbn_to_isbn_encoded)

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}
print('Encoded Angka ke ISBN: ', isbn_encoded_to_isbn)

"""### Mapping"""

# Mapping User_ID ke dataframe user
collab_ratings['user'] = collab_ratings.User_ID.map(user_to_user_encoded)

# Mapping placeID ke dataframe book_title
collab_ratings['book_title'] = collab_ratings.ISBN.map(isbn_to_isbn_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)

# Mendapatkan jumlah book_titles
num_book_titles = len(isbn_to_isbn_encoded)

# Mengubah rating menjadi nilai float
collab_ratings.Book_Rating = collab_ratings.Book_Rating.values.astype(np.float32)

# Nilai minimum rating
min_rating = min(collab_ratings.Book_Rating)

# Nilai maksimal rating
max_rating = max(collab_ratings.Book_Rating)

print('Number of Users: {}\nNumber of Books: {}\nMin Rating: {}\nMax Rating: {}'.format(
    num_users, num_book_titles, min_rating, max_rating
))

"""# Modeling

## Model Development Content-Based Filtering

### TF-IDF Vectorizer
"""

data_content = books_new
data_content.sample(5)

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(data_content['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data_content['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan nama pengarang
# Baris diisi dengan nama buku

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data_content.book_title
).sample(20, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim_content = cosine_similarity(tfidf_matrix)
cosine_sim_content

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim_content, index=data_content['book_title'], columns=data_content['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap judul buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def book_recommendations(judul_buku, similarity_data=cosine_sim_df, items=data_content[['book_title', 'book_author', 'image_url_l']], k=5):
    """
    Rekomendasi Buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    judul_buku : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

test_judul_buku = 'The Foxman'

data_content[data_content.book_title.eq(test_judul_buku)]

# Mendapatkan rekomendasi buku yang mirip dengan 'Mistaken Identity'
book_recommendations(test_judul_buku)

"""## Model Development Collaborative Filtering dengan Popular Based

### Cosine Similarity
"""

# menghitung kemiripan antar vektor menggunakan metode yang disebut kesamaan kosinus.
cosine_sim_collab = cosine_similarity(pivot_data)

"""### Mendapatkan Rekomendasi"""

def recommend_book(book_name):
    # Temukan indeks nama buku dari variabel input 'book_name' di tabel pivot
    index = np.where(pivot_data.index == book_name)[0][0]

    # Mengambil skor similarity buku masukan dengan buku lain,
    # urutkan dalam urutan descending, dan pilih 4 item yang sama teratas
    similar_items = sorted(list(enumerate(cosine_sim_collab[index])), key=lambda x: x[1], reverse=True)[1:5]

    data = []
    for i in similar_items:
        item = []

        # Mengambil detail buku yang relevan (judul, penulis, URL gambar) dari kumpulan data 'buku'
        temp_df = books[books['Book_Title'] == pivot_data.index[i[0]]]

        item.extend(list(temp_df.drop_duplicates('Book_Title')['Book_Title'].values))
        item.extend(list(temp_df.drop_duplicates('Book_Title')['Book_Author'].values))
        item.extend(list(temp_df.drop_duplicates('Book_Title')['Image_URL_L'].values))

        data.append(item)

    return data

temp_data = new_data.drop(labels=['User_ID', 'Book_Rating'], axis=1).drop_duplicates('ISBN')

test_buku = 'You Belong To Me'
temp_data[temp_data.Book_Title.eq(test_buku)]

recommend_book('You Belong To Me')

"""## Model Development Collaborative Filtering dengan User Based

### Membagi Data untuk Training dan Validasi
"""

# Mengacak dataset
collab_ratings = collab_ratings.sample(frac=1, random_state=42)
collab_ratings

# Membuat variabel x untuk mencocokkan data user dan book_title menjadi satu value
x = collab_ratings[['user', 'book_title']].values

# Membuat variabel y untuk membuat rating dari hasil
y = collab_ratings.Book_Rating.apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * collab_ratings.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### Proses Training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book_title, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book_title = num_book_title
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_book_title,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_book_title, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book_titles, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""### Mendapatkan Rekomendasi"""

book_df = books_new

# Mengambil sample user
user_id = collab_ratings.User_ID.sample(1).iloc[0]
book_readed_by_user = collab_ratings[collab_ratings.User_ID == user_id]

book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user.ISBN.values)]['isbn']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_readed = [[isbn_to_isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_collab_model = model.predict(user_book_array).flatten()

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_readed_by_user.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
book_rating_from_user_data = []
for row in book_df_rows.itertuples():
  book_rating_from_user_data.append([row.book_title, row.book_author])

df_book_rating_from_user = pd.DataFrame(book_rating_from_user_data, columns=['Book Title', 'Book Author'])
print(df_book_rating_from_user)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

top_ratings_indices = ratings_collab_model.argsort()[-10:][::-1]
recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]
recommend_book_data = []
for row in recommended_book.itertuples():
  recommend_book_data.append([row.book_title, row.book_author, row.image_url_l])

df_recommend_book_data = pd.DataFrame(recommend_book_data, columns=['Book Title', 'Book Author', 'Image'])
df_recommend_book_data

"""# Evalution

## Model Evaluasi dengan Content-Based Filtering
"""

# Menentukan threshold untuk mengkategorikan kesamaan sebagai 1 atau 0
threshold = 0.5

# Buat data ground_truth dengan asumsi threshold
ground_truth_content = np.where(cosine_sim_content >= threshold, 1, 0)

# Menampilkan beberapa sampel nilai ground_truth_content_df dalam matriks
ground_truth_content_df = pd.DataFrame(ground_truth_content, index=data_content['book_title'], columns=data_content['book_title']).sample(5, axis=1).sample(10, axis=0)
ground_truth_content_df

# Mengambil sebagian kecil dari matriks cosine_sim_content dan matriks kebenaran dasar
sample_size = 10000
cosine_sim_sample_content = cosine_sim_content[:sample_size, :sample_size]
ground_truth_sample_content = ground_truth_content[:sample_size, :sample_size]

# Mengonversi matriks sample cosine similarity menjadi array satu dimensi untuk perbandingan
cosine_sim_flat_content = cosine_sim_sample_content.flatten()

# Mengonversi matriks sample ground truth menjadi array satu dimensi untuk perbandingan
ground_truth_flat_content = ground_truth_sample_content.flatten()

# Hitung metrik evaluasi
predictions_content = (cosine_sim_flat_content >= threshold).astype(int)
precision_content, recall_content, f1_scores_content, _ = precision_recall_fscore_support(
     ground_truth_flat_content, predictions_content, average='binary', zero_division=1
)

print("Precision:", precision_content)
print("Recall:", recall_content)
print("F1-score:", f1_scores_content)

"""## Model Evaluasi Collaborative Filtering dengan Popular Based"""

# Menentukan threshold untuk mengkategorikan kesamaan sebagai 1 atau 0
threshold = 0.5

# Buat data ground_truth dengan asumsi threshold
ground_truth_collab = np.where(cosine_sim_collab >= threshold, 1, 0)

# Mengambil sebagian kecil dari matriks cosine_sim_content dan matriks kebenaran dasar
sample_size = 350
cosine_sim_sample_collab= cosine_sim_collab[:sample_size, :sample_size]
ground_truth_sample_collab = ground_truth_collab[:sample_size, :sample_size]

# Mengonversi matriks sample cosine similarity menjadi array satu dimensi untuk perbandingan
cosine_sim_flat_collab = cosine_sim_sample_collab.flatten()

# Mengonversi matriks sample ground truth menjadi array satu dimensi untuk perbandingan
ground_truth_flat_collab = ground_truth_sample_collab.flatten()

# Hitung metrik evaluasi
predictions_collab = (cosine_sim_flat_collab >= threshold).astype(int)
precision_collab, recall_collab, f1_scores_collab, _ = precision_recall_fscore_support(
     ground_truth_flat_collab, predictions_collab, average='binary', zero_division=1
)

print("Precision:", precision_collab)
print("Recall:", recall_collab)
print("F1-score:", f1_scores_collab)

"""## Model Evaluasi Collaborative Filtering dengan User Based"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()